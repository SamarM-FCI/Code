# -*- coding: utf-8 -*-
"""BI-LSTM_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CsnJtYSZgs4cvlrgT86nFCMdyAJLFvg6
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
train = pd.read_csv('/content/drive/My Drive/Master_Finally/Fake_vaccination/Data/train_data.csv')
val = pd.read_csv('/content/drive/My Drive/Master_Finally/Fake_vaccination/Data/val_data.csv')
test = pd.read_csv('/content/drive/My Drive/Master_Finally/Fake_vaccination/Data/test_data.csv')
data = pd.read_csv('/content/drive/My Drive/Master_Finally/Fake_vaccination/Data/data_after_cleaning.csv')

data.head()

#train _ test split
from sklearn.model_selection import train_test_split

X = data['root_extracting']
y = data['stance']
train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)



# BLSTM
import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Embedding, Dense, Bidirectional
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Sample Arabic text dataset
texts= train_X
labels = train_y

# Tokenization and padding
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences)

# Label encoding
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels)

X_train, X_val, y_train, y_val = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)

# Build the LSTM model
embedding_dim = 50
vocab_size = len(tokenizer.word_index) + 1

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=X_train.shape[1]))
model.add(Bidirectional(LSTM(100)))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, epochs=10, batch_size=218, validation_data=(X_val, y_val))

#predictions_new = model.predict(X_val)
# If you want binary predictions (0 or 1)
binary_predictions_new = (predictions_new > 0.5).astype(int)

# If you want class labels
class_labels_new = label_encoder.inverse_transform(binary_predictions_new.flatten())

predictions = class_labels_new
val_y = y_val

from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# Assuming val_y is your true labels and predictions are your predicted labels

# Print Classification Report
print("Classification Report:")
print(classification_report(val_y, predictions))

# Calculate and Print ROC-AUC Score
roc_auc = roc_auc_score(val_y, predictions)
print(f"ROC-AUC Score: {roc_auc:.4f}")

# Calculate and Print Accuracy
accuracy = accuracy_score(val_y, predictions)
print(f"Accuracy: {accuracy:.4f}")

# Calculate and Print F1 Score
f1 = f1_score(val_y, predictions)
print(f"F1 Score: {f1:.4f}")

# Calculate and Print Precision
precision = precision_score(val_y, predictions)
print(f"Precision: {precision:.4f}")

# Calculate and Print Recall
recall = recall_score(val_y, predictions)
print(f"Recall: {recall:.4f}")

# Plot ROC Curve
fpr, tpr, thresholds = roc_curve(val_y, predictions)
roc_auc_curve = auc(fpr, tpr)

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_curve))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

import matplotlib.pyplot as plt

# Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Assuming 'history' is the object returned by model.fit()

# Extract training accuracy from the history object
training_accuracy = history.history['accuracy']

# Number of epochs
epochs = range(1, len(training_accuracy) + 1)

# Plot training accuracy
plt.plot(epochs, training_accuracy, label='Training Accuracy', marker='o')
plt.title('Training Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()



test = pd.read_csv("predictionsLSTM_v1.csv")

# Sample Arabic text dataset
texts_test = test['Text']
labels_test = test['Actual_Label']


# Assuming X_new is your new data (similarly tokenized and padded)
sequences_new = tokenizer.texts_to_sequences(texts_test)
padded_sequences_new = pad_sequences(sequences_new, maxlen=X_train.shape[1])

# Predictions for the new data
predictions_new = model.predict(padded_sequences_new)

# If you want binary predictions (0 or 1)
binary_predictions_new = (predictions_new > 0.5).astype(int)

# If you want class labels
class_labels_new = label_encoder.inverse_transform(binary_predictions_new.flatten())

from sklearn.metrics import accuracy_score
import pandas as pd

# Assuming label_encoder is used to convert class labels to binary format (0 or 1)
binary_labels_test = label_encoder.transform(labels_test)

# Evaluate accuracy
accuracy = accuracy_score(binary_labels_test, binary_predictions_new)
print(f'Accuracy on the test set: {accuracy * 100:.2f}%')