# -*- coding: utf-8 -*-
"""DA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1khrJC8gzsOwqOJ7n9t0QXhZ516qXlhcm

### TEXT translation

##### trans to english
"""

### model from hugging face

from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M")

#### from google
import pandas as pd
from googletrans import Translator
import os

translator = Translator()

def translateToEn(text):
    translation = translator.translate(text, src='ar', dest='en')
    return translation.text

# from Hug face
def transToEng(text) :
    tokenizer.src_lang = "ar"
    encoded_zh = tokenizer(text, return_tensors="pt")
    generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("en"))
    translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    return translated_text

import pandas as pd
df = pd.read_csv("fakeData.csv")
df.head()

# part1
output_file_path = 'trans.csv'
df['Translated_Text'] = df['cleanText'].apply(translateToEn)
df.to_csv(output_file_path, index=False)

"""#### back translation"""

def transToArab(text):
    tokenizer.src_lang = "en"
    encoded_zh = tokenizer(text, return_tensors="pt")
    generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id("ar"))
    translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
    return translated_text

df = pd.read_csv("trans/tran.csv")
df.head()

# part1
output_file_path = 'backtran.csv'
df['Translated_Text'] = df['cleanText'].apply(transToArab)
df.to_csv(output_file_path, index=False)

data = pd.read_csv('backtranP.csv')
data.head()

### from translate lib
from translate import Translator
import pandas as pd


def translate_text(text):
    translator = Translator(to_lang='ar', from_lang='en')
    translation = translator.translate(text)
    return translation

# part2
df2 = pd.read_csv('trans/tranP2.csv')
output_file_path = 'back2.csv'
df2['Translated_Text'] = df2['cleanText'].apply(translate_text)
df2.to_csv(output_file_path, index=False)

output_file_path = 'backtranP3.csv'
df3['Translated_Text'] = df3['cleanText'].apply(translateToAr)
df3.to_csv(output_file_path, index=False)

"""### synonumous replacement (english)"""

import nltk
from nltk.corpus import wordnet
import random

import nltk
import random
from nltk.corpus import wordnet

nltk.download("punkt")
nltk.download("wordnet")

def synonym_replacement(text, n=1):
    words = nltk.word_tokenize(text)
    augmented_sentence = words.copy()

    for _ in range(n):
        for j in range(len(words)):
            synsets = wordnet.synsets(words[j])
            if synsets:
                synonym = random.choice(synsets).lemmas()[0].name()
                augmented_sentence[j] = synonym

    augmented_text = ' '.join(augmented_sentence)
    return augmented_text

data = pd.read_csv("fakeData.csv")
data.cleanText[2], synonym_replacement(data.cleanText[2])

data = pd.read_csv("trans/tranP1.csv")
column_name = 'Translated_Text'
n =1
data[column_name] = data[column_name].apply(lambda x: synonym_replacement(x, n))

# Save the augmented dataset to a new CSV file
augmented_file = 'synEngP1.csv'
data.to_csv(augmented_file, index=False, encoding='utf-8')

print(f'Augmented dataset saved to {augmented_file}')

data = pd.read_csv("trans/tranP3.csv")
column_name = 'Translated_Text'
n =1
data[column_name] = data[column_name].apply(lambda x: synonym_replacement(x, n))

# Save the augmented dataset to a new CSV file
augmented_file = 'synEngP3.csv'
data.to_csv(augmented_file, index=False, encoding='utf-8')

print(f'Augmented dataset saved to {augmented_file}')

from googletrans import Translator
import os
translator = Translator()

def translateToAr(text):
    translation = translator.translate(text, src='en', dest='ar')
    return translation.text

# back to arabic
df1 = pd.read_csv('synon/synEngP1.csv')
# part1
output_file_path = 'synArbP1.csv'
df1['arbSyn_Text'] = df1['Translated_Text'].apply(translateToAr)
df1.to_csv(output_file_path, index=False)

# part2
df1 = pd.read_csv('synon/synEngP3.csv')
output_file_path = 'synArbP3.csv'
df1['arbSyn_Text'] = df1['Translated_Text'].apply(translateToAr)
df1.to_csv(output_file_path, index=False)

# part3
output_file_path = 'synArbP3.csv'
df3['Translated_Text'] = df3['augmentedText'].apply(translateToAr)
df3.to_csv(output_file_path, index=False)

### check <<<==========
data = pd.read_csv("synon/synArbP1.csv")
data.cleanText[2] , data.Translated_Text[2], data.arbSyn_Text[2]

"""## Random deletetion"""

import re
import string

def SentProcessing(sent):
    # Remove Arabic diacritics (tashkeel)
    sent = re.sub('[\u0617-\u061A\u064B-\u0652]', '', sent)

    # Remove Arabic punctuation
    sent = sent.translate(str.maketrans('', '', string.punctuation))

    # Remove extra whitespaces and convert to lowercase
    sent = ' '.join(sent.split()).lower()

    return sent

import random

def Random_Deletion_Arabic(sent, probability):
    '''Randomly remove each word in the sentence with probability p'''
    psent = SentProcessing(sent)
    sent_list = psent.split()

    if len(sent_list) == 1:
        return ''.join(sent_list)

    new_sents = []
    for word in sent_list:
        r = random.uniform(0, 1)
        if r > probability:
            new_sents.append(word)

    if len(new_sents) == 0:
        rand_int = random.randint(0, len(sent_list) - 1)
        return ''.join([sent_list[rand_int]])

    return ' '.join(new_sents)

import pandas as pd

input_file_path = "fakeData.csv"
output_file_path = 'randDel_FD2.csv'  # Define the path for the augmented dataset

# Load the dataset into a DataFrame (assuming it has a 'Text' column)
data = pd.read_csv(input_file_path)

# Define the probability for random deletion
probability = 0.4  # Adjust the probability as needed

# Create a list to store augmented sentences
augmented_sentences = []

# Iterate through the dataset and apply random deletion
for index, row in data.iterrows():
    arabic_text = row['cleanText']
    augmented_text = Random_Deletion_Arabic(SentProcessing(arabic_text), probability)
    augmented_sentences.append({'Original_Text': arabic_text, 'Augmented_Text': augmented_text})

# Create a new DataFrame from the augmented data
augmented_df = pd.DataFrame(augmented_sentences)

# Save the augmented dataset to a CSV file
augmented_df.to_csv(output_file_path, index=False)

"""## Random swap"""

import pandas as pd
import random

# Define a function to perform random word swap
def random_word_swap(text, n=2):
    words = text.split()
    if len(words) < 2:
        return text
    else:
        for _ in range(n):
            idx1, idx2 = random.sample(range(len(words)), 2)
            words[idx1], words[idx2] = words[idx2], words[idx1]
        return ' '.join(words)

import pandas as pd
data = pd.read_csv('randDelFinal.csv')
data = data[0:1000]
column_name = 'Augmented_Text'
data[column_name] = data[column_name].apply(random_word_swap)

# Save the augmented dataset to a new CSV file
augmented_file = 'addPart.csv'
data.to_csv(augmented_file, index=False, encoding='utf-8')

print(f'Augmented dataset saved to {augmented_file}')

data = pd.read_csv('trans/backtran.csv')
column_name = 'Translated_Text'
data[column_name] = data[column_name].apply(random_word_swap)

# Save the augmented dataset to a new CSV file
augmented_file = 'ranSwap_backtran.csv'
data.to_csv(augmented_file, index=False, encoding='utf-8')

print(f'Augmented dataset saved to {augmented_file}')

data = pd.read_csv('randDel/randDel_backtran.csv')
column_name = 'Augmented_Text'
data[column_name] = data[column_name].apply(random_word_swap)

# Save the augmented dataset to a new CSV file
augmented_file = 'ranSwap_randDel_backtran.csv'
data.to_csv(augmented_file, index=False, encoding='utf-8')

print(f'Augmented dataset saved to {augmented_file}')

data = pd.read_csv('randDel/randDel_dataset.csv')
column_name = 'Augmented_Text'
data[column_name] = data[column_name].apply(random_word_swap)

# Save the augmented dataset to a new CSV file
augmented_file = 'ranSwap_randDel_dataset.csv'
data.to_csv(augmented_file, index=False, encoding='utf-8')

print(f'Augmented dataset saved to {augmented_file}')