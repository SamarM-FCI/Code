# -*- coding: utf-8 -*-
"""PRE-PROCESS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Kxf_23jh19DEhN_rDMsETWoqccNQw2_
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
train = pd.read_csv('/content/drive/My Drive/Master_Finally/Fake_vaccination/pre-processing steps/Data/final_dataset.csv')
data.head()

import seaborn as sb
data['text_len'] = data.loc[:,'cleanText'].apply(lambda x:len(x.split()))
sb.displot(data.text_len,bins=300)

"""## ***Tokenization***"""

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
data['tokenized_content'] = data.cleanText.apply(word_tokenize)
display(data)

"""##***Lemmatization***"""

pip install stanza

import stanza

# Download the Arabic language model
stanza.download('ar')

# Initialize the pipeline with tokenization and lemmatization
nlp = stanza.Pipeline('ar', processors='tokenize,lemma')

# Lemmatization function
def lemmatize_arabic_text(text):
    doc = nlp(text)
    lemmatized_text = ' '.join([word.lemma for sentence in doc.sentences for word in sentence.words])
    return lemmatized_text

# Assuming your dataset has a column named 'arabic_text'
# Apply lemmatization to the 'arabic_text' column
data['lemmatized_text'] = data.cleanText.apply(lemmatize_arabic_text)

# Display the results
print(data)

'''from nltk.stem.isri import ISRIStemmer
stemmer = ISRIStemmer()'''

from nltk.stem.arlstem import ARLSTem
stemmer = ARLSTem()

def stem(text):
    stemmed = []
    for word in text:
        stemmed.append(stemmer.stem(word))
    return stemmed

def stem_untokenized(text):
    return stemmer.stem(text)

data['stemming'] = data.tokenized_content.apply(stem)
display(data)

# save
import pandas as pd

# Assuming data is your original DataFrame
train_data = pd.concat([data['root_extracting'], data['cleanText'], data['stance']], axis=1)

# Save the updated DataFrame to a CSV file
train_data.to_csv('dataset_final.csv', index=False)

"""## TFIDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt

X = data['root_extracting']
VecModel = TfidfVectorizer()
X_Vec = VecModel.fit_transform(X)
X_Vec = pd.DataFrame.sparse.from_spmatrix(X_Vec)
print(f'The new shape for X is {X_Vec.shape}')

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_Vec, data['stance'], test_size=0.3, random_state=42)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=1000)
model = LogisticRegression(max_iter=1000, solver='saga', tol=1e-3)

model.fit(X_train, y_train)

y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

#try
import numpy as np
from keras.models import Sequential
from keras.layers import LSTM, Embedding, Dense, Bidirectional
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Sample Arabic text dataset
texts = data['root_extracting']
labels = data['stance']

# Tokenization and padding
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)
sequences = tokenizer.texts_to_sequences(texts)
padded_sequences = pad_sequences(sequences)

# Label encoding
label_encoder = LabelEncoder()
encoded_labels = label_encoder.fit_transform(labels)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, encoded_labels, test_size=0.2, random_state=42)

# Build the LSTM model
embedding_dim = 50
vocab_size = len(tokenizer.word_index) + 1

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=X_train.shape[1]))
model.add(Bidirectional(LSTM(100)))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=2, batch_size=1, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")